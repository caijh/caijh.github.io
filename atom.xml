<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://caijh.github.io/</id>
    <title>John&apos;s Blog</title>
    <updated>2020-01-01T11:41:41.535Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://caijh.github.io/"/>
    <link rel="self" href="https://caijh.github.io/atom.xml"/>
    <subtitle>搬砖小弟</subtitle>
    <logo>https://caijh.github.io/images/avatar.png</logo>
    <icon>https://caijh.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, John&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes - 通过kubeadm部署Kubernetes集群]]></title>
        <id>https://caijh.github.io/post/kubernetes-tong-guo-kubeadm-bu-shu-kubernetes-ji-qun</id>
        <link href="https://caijh.github.io/post/kubernetes-tong-guo-kubeadm-bu-shu-kubernetes-ji-qun">
        </link>
        <updated>2020-01-01T11:19:54.000Z</updated>
        <content type="html"><![CDATA[<h1 id="准备环境">准备环境</h1>
<p>使用vagrant来生成三台虚拟机，虚拟机基本信息如下：</p>
<table>
<thead>
<tr>
<th>hostname</th>
<th>ip</th>
<th>操作系统</th>
</tr>
</thead>
<tbody>
<tr>
<td>kube-master</td>
<td>192.168.33.10</td>
<td>centos/7</td>
</tr>
<tr>
<td>kube-node-1</td>
<td>192.168.33.11</td>
<td>centos/7</td>
</tr>
<tr>
<td>kube-node-2</td>
<td>192.168.33.12</td>
<td>centos/7</td>
</tr>
</tbody>
</table>
<p>Vagrantfile配置内容如下：</p>
<pre><code># -*- mode: ruby -*-
# vi: set ft=ruby :

# All Vagrant configuration is done below. The &quot;2&quot; in Vagrant.configure
# configures the configuration version (we support older styles for
# backwards compatibility). Please don't change it unless you know what
# you're doing.
Vagrant.configure(&quot;2&quot;) do |config|
  # The most common configuration options are documented and commented below.
  # For a complete reference, please see the online documentation at
  # https://docs.vagrantup.com.

  # Every Vagrant development environment requires a box. You can search for
  # boxes at https://vagrantcloud.com/search.
  config.vm.box = &quot;centos/7&quot;
  config.vm.hostname = &quot;k8s-master&quot;

  # Disable automatic box update checking. If you disable this, then
  # boxes will only be checked for updates when the user runs
  # `vagrant box outdated`. This is not recommended.
  # config.vm.box_check_update = false

  # Create a forwarded port mapping which allows access to a specific port
  # within the machine from a port on the host machine. In the example below,
  # accessing &quot;localhost:8080&quot; will access port 80 on the guest machine.
  # NOTE: This will enable public access to the opened port
  config.vm.network &quot;forwarded_port&quot;, guest: 8001, host: 8001

  # Create a forwarded port mapping which allows access to a specific port
  # within the machine from a port on the host machine and only allow access
  # via 127.0.0.1 to disable public access
  # config.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8080, host_ip: &quot;127.0.0.1&quot;

  # Create a private network, which allows host-only access to the machine
  # using a specific IP.
  config.vm.network &quot;private_network&quot;, ip: &quot;192.168.33.10&quot;

  # Create a public network, which generally matched to bridged network.
  # Bridged networks make the machine appear as another physical device on
  # your network.
  # config.vm.network &quot;public_network&quot;

  # Share an additional folder to the guest VM. The first argument is
  # the path on the host to the actual folder. The second argument is
  # the path on the guest to mount the folder. And the optional third
  # argument is a set of non-required options.
  config.vm.synced_folder &quot;../data&quot;, &quot;/vagrant_data&quot;

  # Provider-specific configuration so you can fine-tune various
  # backing providers for Vagrant. These expose provider-specific options.
  # Example for VirtualBox:
  #
  config.vm.provider &quot;virtualbox&quot; do |vb|
    # Display the VirtualBox GUI when booting the machine
    vb.gui = false
  
    # Customize the amount of memory on the VM:
    vb.memory = &quot;2048&quot;
    vb.cpus = 2
  end
  #
  # View the documentation for the provider you are using for more
  # information on available options.

  # Enable provisioning with a shell script. Additional provisioners such as
  # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the
  # documentation for more information about their specific syntax and use.
  # config.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL
  #   apt-get update
  #   apt-get install -y apache2
  # SHELL
end

</code></pre>
<p>以上只是kube-master的配置cpus不小于2，其他人配置根据需要修改，如hostname,vb. memory,private_network</p>
<h1 id="安装">安装</h1>
<p><strong>各个节点都需要的操作</strong></p>
<ol>
<li>
<p>关闭防火墙selinux</p>
<pre><code># systemctl stop firewalld
# systemctl disable firewalld
# Set SELinux in permissive mode (effectively disabling it)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
</code></pre>
</li>
<li>
<p>关闭swap交换区功能，实际不用也行，但是如果不关闭的话，kubeadm等会报错虽然也可以通过命令行加参数不让报错，但没有直接关闭来的方便</p>
<p>临时关闭，可以使用命令<code>swapoff -a</code></p>
<p>永远关闭，直接编辑<code>/etc/fstab</code>, 将swapfile那行注释掉</p>
</li>
<li>
<p>安装docker</p>
<pre><code>yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce docker-ce-cli containerd.io
systemctl enable docker --now
</code></pre>
</li>
<li>
<p>网络增强</p>
<p>创建/etc/sysctl.d/k8s.conf文件，添加如下内容：</p>
<pre><code>net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
</code></pre>
<p>执行如下命令使修改生效：</p>
<pre><code>$ modprobe br_netfilter
$ sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>
</li>
<li>
<p>安装有kubeadm</p>
<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
<pre><code>yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl enable --now kubelet
</code></pre>
<p>注：因为使用了vagrant来搭建k8s集群，这里还需要修改/etc/sysconfig/kubelet,<code>KUBELET_EXTRA_ARGS=&quot;--node-ip=&lt;node_ip&gt;&quot;</code>,否则会报以下错误</p>
<p>![image-20191215223528931](Kubernetes - kubeadm安装k8s集群.assets/image-20191215223528931.png)</p>
</li>
<li>
<p>准备kubeadm init 过程中使用的镜像</p>
<p>由于k8s.gcr.io被墙的原因，镜像没有直接pull到， 需要先从别的地方（registry.cn-hangzhou.aliyuncs.com）拉到到镜像，再tag成需要的镜像, 以下是我要部署的镜像；</p>
<pre><code>REPOSITORY                                                                    TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                                                         v1.17.0             7d54289267dc        7 days ago          116MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy                v1.17.0             7d54289267dc        7 days ago          116MB
k8s.gcr.io/kube-controller-manager                                            v1.17.0             5eb3b7486872        7 days ago          161MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager   v1.17.0             5eb3b7486872        7 days ago          161MB
k8s.gcr.io/kube-apiserver                                                     v1.17.0             0cae8d5cc64c        7 days ago          171MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver            v1.17.0             0cae8d5cc64c        7 days ago          171MB
k8s.gcr.io/kube-scheduler                                                     v1.17.0             78c190f736b1        7 days ago          94.4MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler            v1.17.0             78c190f736b1        7 days ago          94.4MB
busybox                                                                       latest              b534869c81f0        11 days ago         1.22MB
kubernetesui/dashboard                                                        v2.0.0-beta6        84cd817d07fb        4 weeks ago         91.7MB
k8s.gcr.io/coredns                                                            1.6.5               70f311871ae1        5 weeks ago         41.6MB
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns                   1.6.5               70f311871ae1        5 weeks ago         41.6MB
k8s.gcr.io/etcd                                                               3.4.3-0             303ce5db0e90        7 weeks ago         288MB
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        7 weeks ago         288MB
kubernetesui/metrics-scraper                                                  v1.0.1              709901356c11        5 months ago        40.1MB
quay.io/coreos/flannel                                                        v0.11.0-amd64       ff281650a721        10 months ago       52.6MB
k8s.gcr.io/pause                                                              3.1                 da86e6ba6ca1        24 months ago       742kB
registry.cn-hangzhou.aliyuncs.com/google_containers/pause                     3.1                 da86e6ba6ca1        24 months ago       742kB
</code></pre>
<p>主节点需要的镜像有：kube-proxy，kube-controller-manager，kube-apiserver，kube-scheduler，coredns，etcd，pause</p>
<p>工作节点需要的镜像有：kube-proxy，pause</p>
</li>
</ol>
<p><strong>Master节点操作</strong></p>
<ol>
<li>执行kubeadm init</li>
</ol>
<pre><code>kubeadm init --apiserver-advertise-address=192.168.33.10 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16
</code></pre>
<p>具体请参考官网，我只设置了apiserver-advertise-address，pod-network-cidr</p>
<pre><code>[init] Using Kubernetes version: vX.Y.Z
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;
[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;
[certs] Generating &quot;etcd/ca&quot; certificate and key
[certs] Generating &quot;etcd/server&quot; certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key
[certs] Generating &quot;etcd/peer&quot; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key
[certs] Generating &quot;ca&quot; certificate and key
[certs] Generating &quot;apiserver&quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [kubeadm-cp kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]
[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key
[certs] Generating &quot;front-proxy-ca&quot; certificate and key
[certs] Generating &quot;front-proxy-client&quot; certificate and key
[certs] Generating &quot;sa&quot; key and public key
[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;
[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file
[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;
[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;
[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;
[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;
[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s
[apiclient] All control plane components are healthy after 31.501735 seconds
[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace
[kubelet] Creating a ConfigMap &quot;kubelet-config-X.Y&quot; in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;kubeadm-cp&quot; as an annotation
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the label &quot;node-role.kubernetes.io/master=''&quot;
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: &lt;token&gt;
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre>
<p>记得执行以下命令</p>
<pre><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<ol start="2">
<li>安装pod 网络插件，我选择了flannel</li>
</ol>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
</code></pre>
<p><strong>节点操作</strong></p>
<ol>
<li>
<p>在master节点执行kubeadm token list获取token</p>
</li>
<li>
<p>获取hash</p>
<pre><code>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \
   openssl dgst -sha256 -hex | sed 's/^.* //'
</code></pre>
</li>
<li>
<p>执行kubeadm join命令</p>
<pre><code>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre>
</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/caijh/imagehosting/master/20200101194030.png" alt="" loading="lazy"></figure>
<h1 id="安装dasboard-admin">安装dasboard-admin</h1>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
</code></pre>
<pre><code>kubectl proxy -address=0.0.0.0
</code></pre>
<p>创建集群管理账号被绑定cluster-admin</p>
<pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-admin
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: dashboard-admin
subjects:
  - kind: ServiceAccount
    name: dashboard-admin
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
</code></pre>
<p>获取token,登录</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/caijh/imagehosting/master/20200101193859.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes - 解决删除namespace一直Terminating的问题]]></title>
        <id>https://caijh.github.io/post/kubernetes-jie-jue-shan-chu-namespace-yi-zhi-terminating-de-wen-ti</id>
        <link href="https://caijh.github.io/post/kubernetes-jie-jue-shan-chu-namespace-yi-zhi-terminating-de-wen-ti">
        </link>
        <updated>2019-12-28T15:04:43.000Z</updated>
        <content type="html"><![CDATA[<p>在删除命名空间时，一直卡在Terminating状态，使用--force参数依然无法删除，报错</p>
<pre><code>Error from server (Conflict): Operation cannot be fulfilled on namespaces &quot;test&quot;: The system is ensuring all content is removed from this namespace. Upon completion, this namespace will automatically be purged by the system.
</code></pre>
<p>可以使用以下方式删除</p>
<ol>
<li><code>kubectl get namespace &lt;annoying-namespace-to-delete&gt; -o json &gt; tmp.json</code></li>
<li>编辑tmp.json文件，删除其中spec字段的finalize字段，如果namespace的meta指令了finalizer也需要删除这信息</li>
<li><code>curl -k -H &quot;Content-Type: application/json&quot; -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/&lt;namspace-to-delete&gt;/finalize</code></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spring Cloud - Eureka]]></title>
        <id>https://caijh.github.io/post/spring-cloud-eureka</id>
        <link href="https://caijh.github.io/post/spring-cloud-eureka">
        </link>
        <updated>2019-11-04T07:00:32.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<p>是由Netflix开源的基于REST的服务治理组件，包含了eureka server 和eureka client。从2012年9月在GitHub上发布1.1.2版本以来，至今已经发布了231次，最新版本为2018年8月份发布的1.9.4版本。期间有进行2.x版本的开发，不过由于各种原因内部已经冻结开发，目前还是以1.x版本为主。Spring Cloud Netflix Eureka是Pivotal公司为了将Netflix Eureka整合于Spring Cloud生态系统提供的版本。</p>
</blockquote>
<h1 id="eureka快速入门">Eureka快速入门</h1>
<p><strong>Eureka Server</strong></p>
<ol>
<li>
<p>引入依赖</p>
<pre><code>        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;
        &lt;/dependency&gt;
</code></pre>
</li>
<li>
<p>主应用类上添加**@EnableEurekaServer**</p>
</li>
<li>
<p>配置eureka</p>
<pre><code>eureka:
  client:
    register-with-eureka: true
    fetch-registry: true
    service-url:
      defaultZone: ${EUREKA_SERVER_ADDRESS:http://localhost:8761/eureka/}
  server:
    peer-node-read-timeout-ms: 5000
</code></pre>
</li>
<li>
<p>启动</p>
<p>![image-20191024165113212](Spring Cloud - 服务注册中心eureka/image-20191024165113212.png)</p>
</li>
</ol>
<p><strong>Eureka Client</strong></p>
<ol>
<li>
<p>引入客户端依赖</p>
<pre><code>        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;
        &lt;/dependency&gt;
</code></pre>
</li>
<li>
<p>主应用类添加**@EnableDiscoveryClient**</p>
</li>
<li>
<p>配置</p>
<pre><code>eureka:
  instance:
    prefer-ip-address: true
  client:
    service-url:
      defaultZone: ${EUREKA_SERVER_ADDRESS:http://localhost:8761/eureka/}
</code></pre>
</li>
<li>
<p>启动客户端</p>
</li>
</ol>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<p>是由Netflix开源的基于REST的服务治理组件，包含了eureka server 和eureka client。从2012年9月在GitHub上发布1.1.2版本以来，至今已经发布了231次，最新版本为2018年8月份发布的1.9.4版本。期间有进行2.x版本的开发，不过由于各种原因内部已经冻结开发，目前还是以1.x版本为主。Spring Cloud Netflix Eureka是Pivotal公司为了将Netflix Eureka整合于Spring Cloud生态系统提供的版本。</p>
</blockquote>
<h1 id="eureka快速入门">Eureka快速入门</h1>
<p><strong>Eureka Server</strong></p>
<ol>
<li>
<p>引入依赖</p>
<pre><code>        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;
        &lt;/dependency&gt;
</code></pre>
</li>
<li>
<p>主应用类上添加**@EnableEurekaServer**</p>
</li>
<li>
<p>配置eureka</p>
<pre><code>eureka:
  client:
    register-with-eureka: true
    fetch-registry: true
    service-url:
      defaultZone: ${EUREKA_SERVER_ADDRESS:http://localhost:8761/eureka/}
  server:
    peer-node-read-timeout-ms: 5000
</code></pre>
</li>
<li>
<p>启动</p>
<p>![image-20191024165113212](Spring Cloud - 服务注册中心eureka/image-20191024165113212.png)</p>
</li>
</ol>
<p><strong>Eureka Client</strong></p>
<ol>
<li>
<p>引入客户端依赖</p>
<pre><code>        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;
        &lt;/dependency&gt;
</code></pre>
</li>
<li>
<p>主应用类添加**@EnableDiscoveryClient**</p>
</li>
<li>
<p>配置</p>
<pre><code>eureka:
  instance:
    prefer-ip-address: true
  client:
    service-url:
      defaultZone: ${EUREKA_SERVER_ADDRESS:http://localhost:8761/eureka/}
</code></pre>
</li>
<li>
<p>启动客户端</p>
</li>
</ol>
<!-- more -->
<h1 id="eureka-高可用">Eureka 高可用</h1>
<p>application-peer1.yml</p>
<pre><code>---
server:
  port: 8761

eureka:
  client:
    register-with-eureka: true # 是否向 eureka server 注册
    fetch-registry: true # 是否向 eureka server 获取其他客户端注册信息
    service-url:
      defaultZone: ${EUREKA_SERVER_ADDRESSES:http://localhost:8762/eureka/}
  server:
    registry-sync-retry-wait-ms: 5000

</code></pre>
<p>application-pee2.yml</p>
<pre><code class="language-yaml">---
server:
  port: 8762

eureka:
  client:
    register-with-eureka: true # 是否向 eureka server 注册
    fetch-registry: true # 是否向 eureka server 获取其他客户端注册信息
    service-url:
      defaultZone: ${EUREKA_SERVER_ADDRESSES:http://localhost:8761/eureka/}
  server:
    registry-sync-retry-wait-ms: 5000

</code></pre>
<p>客户端配置</p>
<pre><code class="language-yaml">eureka:
  client:
    serviceUrl:
      defaultZone: http://localhost:8761/eureka/,http://localhost:8762/eureka/
    register-with-eureka: true
    fetch-registry: true
</code></pre>
<p>启动两个server组成集群，客户配置seviceUrl为集群地址</p>
<ol>
<li>在两个server都没有启动的情况下，启动client会报错，但还是能启动起来的</li>
<li>部分server启动的情况下，client是可以注册到集群的并拉取到服务列表</li>
</ol>
<p>server采用点对点复制方式，高可用主要是在客户端实现的</p>
<ol>
<li>在Client启动之前，如果没有Eureka Server，则可以通过配置eureka.client.backup-registry-impl从备份registry读取关键服务的信息。(一般不会设置，应该是搭建eureka server集群）</li>
<li>在client启动之后，若运行时出现Eureka Server全部挂掉的情况：本地内存有localRegion之前获取的数据，在Eureka Server都不可用的情况下，从Server端定时拉取注册信息回来更新的线程CacheRefreshThread会执行失败，本地localRegion信息不会被更新。</li>
<li>Client端维护了一个Eureka Server的不可用列表，一旦请求发生Connection error或者5xx的情况则会被列入该列表，当该列表的大小超过指定阈值则会重新清空。在重新清空的情况下，Client默认是采用RetryableEurekaHttpClient进行请求，numberOfRetries为3，因此也能够在一定程度保障Client的高可用。</li>
</ol>
<h1 id="eureka-设计原理">Eureka 设计原理</h1>
<p>作为服务注册中心，主要解决以下问题：</p>
<ol>
<li>
<p>客户端怎么向服务注册中心注册？</p>
<p>服务启动时，调用Eureka Server的REST API的register方法，去注册该应用实例的信息。</p>
</li>
<li>
<p>服务实例怎么从服务注册中心中剔除</p>
<p>正常情况下，服务在关闭时通过钩子方法或其他生命周期回调方法去调用Eureka Server的REST API的de-register方法，来删除自身服务实例的信息。为了解决异常情况下，服务没有及时从服务中剔除，Eureka Server要求客户端每隔一段时间续约，发送心跳。如果租约超过一定时间没有进行续约操作，Eureka Server端会主动剔除服务。</p>
</li>
<li>
<p>服务实例数据的一致性问题</p>
<p>服务注册中心微服务中重要的组件，在生产环境中必然是集群部署，两个服务注册中心上的服务列表数据怎么保持一致呢？</p>
<p>下面从四个方面来解释：</p>
<ul>
<li>
<p>AP优于CP</p>
<p>对于分布式系统来说，一般网络条件相对不可控，出现网络分区是不可避免的，因此系统必须具备分区容忍性。在这个前提下分布式系统的设计则在AP及CP之间进行选择。Eureka是在部署在AWS的背景下设计的，其设计者认为，在云端，特别是在大规模部署的情况下，失败是不可避免的，可能因为Eureka自身部署失败，注册的服务不可用，或者由于网络分区导致服务不可用，因此不能回避这个问题。要拥抱这个问题，就需要Eureka在网络分区的时候，还能够正常提供服务注册及发现功能，因此Eureka选择满足Availability这个特性。在实际生产实践中，服务注册及发现中心保留可用及过期的数据总比丢失掉可用的数据好。</p>
</li>
<li>
<p>Peer to Peer</p>
<p>Eureka Server本身依赖了Eureka Client，也就是每个Eureka Server是作为其他Eureka Server的Client。在单个Eureka Server启动的时候，会有一个syncUp的操作，通过Eureka Client请求其他Eureka Server节点中的一个节点获取注册的应用实例信息，然后复制到其他peer节点。</p>
<p>Eureka Server在执行复制操作的时候，使用HEADER_REPLICATION的http header来将这个请求操作与普通应用实例的正常请求操作区分开来。通过HEADER_REPLICATION来标识是复制请求，这样其他peer节点接收到请求的时候，就不会再对它的peer节点进行复制操作，从而避免死循环。</p>
</li>
<li>
<p>Region及Zone设计</p>
<p>由于Netflix的服务大部分是部署在amazon上，因此Eureka的设计一部分也是基于zmazon的region及zone的基础设施上。</p>
<p>Region可以理解为地理上的分区，如亚洲地区、华北地区；Zone可以理解为region内的具体机房，比如说 region 划分为华北地区，然后华北地区有两个机房，就可以在此 region 之下划分出 zone1、zone2 两个 zone。服务最好是注册到同一zone的注册中心，因为如果不是同一zone,可能心跳检测有会问题。服务调用是优先调用同一zone的服务的。</p>
<p>Eureka Server原生支持了Region及AvailabilityZone，由于资源在Region之间默认是不会复制的，因此Eureka Server的高可用主要就在于Region下面的AvailabilityZone。</p>
<p>Eureka Client支持preferSameZone，也就是获取Eureka Server的serviceUrl优先拉取跟应用实例同处于一个AvailabilityZone的Eureka Server地址列表。一个AvailabilityZone可以设置多个Eureka Server实例，它们之间构成peer节点，然后采用Peer to Peer的复制模式。</p>
</li>
<li>
<p>自我保护模式</p>
<p>在分布式系统中通常是要对服务实例进行存活检测的，在发生网络抖动或暂时不可用时可能会误判， 极端情况可能造成注册中心清空服务列表。因此Eureka使用保护模式。</p>
<p>Eureka Client端与Server端之间有个租约，Client要定时发送心跳来维持这个租约，表示自己还存活着。Eureka通过当前注册的实例数，去计算每分钟应该从应用实例接收到的心跳数，如果最近一分钟接收到的续约的次数小于等于指定阈值的话，则关闭租约失效剔除，禁止定时任务剔除失效的实例，从而保护注册信息。</p>
<p>关闭自我保护模样:<code>eureka.server.enable-self-preservation=true</code></p>
</li>
</ul>
</li>
</ol>
<h1 id="eureka-核心类">Eureka 核心类</h1>
<p><strong>InstanceInfo</strong></p>
<p>Eureka使用InstanceInfo类代表注册的服务实例。其主要字段有：</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>instanceId</td>
<td>实例id</td>
</tr>
<tr>
<td>appName</td>
<td>应用名</td>
</tr>
<tr>
<td>ipAddr</td>
<td>服务的ip地址</td>
</tr>
<tr>
<td>port</td>
<td>端口</td>
</tr>
<tr>
<td>datacenterInfo</td>
<td>DataCenterInfo（Netflix, Amazon, MyOwn）</td>
</tr>
<tr>
<td>leaseInfo</td>
<td>LeaseInfo租约信息</td>
</tr>
<tr>
<td>Status</td>
<td>InstanceStatus服务实例信息</td>
</tr>
<tr>
<td>lastUpdatedTimestamp</td>
<td>上一次更新时间</td>
</tr>
<tr>
<td>lastDirtyTimestamp</td>
<td>上一次失效时间</td>
</tr>
<tr>
<td>metadata</td>
<td>元数据信息</td>
</tr>
</tbody>
</table>
<p><strong>LeaseInfo</strong></p>
<p>Eureka使用LeaseInfo表示应用服务实例的租约信息，其主要字段有：</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>serviceUpTimestamp</td>
<td>服务启动的时间戳</td>
</tr>
<tr>
<td>registrationTimestamp</td>
<td>服务注册的时间戳</td>
</tr>
<tr>
<td>lastRenewalTimestamp</td>
<td>上一次续租时间戳</td>
</tr>
<tr>
<td>evictionTimestamp</td>
<td>被剔除的时间戳</td>
</tr>
<tr>
<td>renewalIntervalInSecs</td>
<td>客户端续租的间隔周期</td>
</tr>
<tr>
<td>durationInSecs</td>
<td>客户端设置的租约的有效时长</td>
</tr>
</tbody>
</table>
<p><strong>InstanceStatus</strong></p>
<p>Eureka使用InstanceStatus表示服务实例的状态。InstanceStatus是枚举类，枚举值有：</p>
<table>
<thead>
<tr>
<th>UP</th>
<th>服务上线</th>
</tr>
</thead>
<tbody>
<tr>
<td>DOWN</td>
<td>服务下线（心跳检测失败）</td>
</tr>
<tr>
<td>STARTING</td>
<td>服务启动中</td>
</tr>
<tr>
<td>OUT_OF_SERVICE</td>
<td>服务关闭（服务自己下线）</td>
</tr>
<tr>
<td>UNKNOWN</td>
<td>未知</td>
</tr>
</tbody>
</table>
<p><strong>ServiceInstance</strong></p>
<p>ServiceInstance是Spring Cloud是service discovery的实例的抽象接口，约定的服务的实例应该有哪些通用信息。由于Spring Cloud Discovery适配了Zookeeper、Consul、Netflix Eureka等注册中心，因此其ServiceInstance定义更为抽象和通用，而且采取的是定义方法的方式。EurekaRegistration实现了ServiceInstance接口。</p>
<p><strong>LeaseManager</strong></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>void register(T r, int leaseDuration, boolean isReplication)</td>
<td>注册新服务</td>
</tr>
<tr>
<td>boolean cancel(String appName, String id, boolean isReplication)</td>
<td>删除服务</td>
</tr>
<tr>
<td>boolean renew(String appName, String id, boolean isReplication)</td>
<td>服务续约</td>
</tr>
<tr>
<td>void evict()</td>
<td>剔除过期的服务，server使用</td>
</tr>
</tbody>
</table>
<p><strong>LookupService</strong></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application getApplication(String appName);</td>
<td>根据appName查找应用</td>
</tr>
<tr>
<td>Applications getApplications();</td>
<td>查询所有应用</td>
</tr>
<tr>
<td>List<InstanceInfo> getInstancesById(String id);</td>
<td>根据id查找服务实例</td>
</tr>
<tr>
<td>InstanceInfo getNextServerFromEureka(String virtualHostname, boolean secure);</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="eureka-核心参数">Eureka 核心参数</h1>
<p><strong>Server与Client共用参数</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>eureka.instance.instanceId</td>
<td></td>
<td>eureka实例id</td>
</tr>
<tr>
<td>eureka.instance.preferIpAddress</td>
<td>flase</td>
<td>是否优先使用ip来代替hostname作为实例的hostName字段的值</td>
</tr>
<tr>
<td>eureka.instance.metadataMap</td>
<td></td>
<td>eureka实例的元数据</td>
</tr>
<tr>
<td>eureka.instance.leaseRenewalIntervalInSeconds</td>
<td>30</td>
<td>eureka client发送心跳的周期</td>
</tr>
<tr>
<td>eureka.instance.leaseExpirationDurationInSeconds</td>
<td>90</td>
<td>eureka server等待client续约的时间</td>
</tr>
</tbody>
</table>
<p>更多参数见<strong>EurekaInstanceConfigBean</strong></p>
<p><strong>Server端(见EurekaServerConfigBean)</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>eureka.server.enableSelfPreservation</td>
<td>True</td>
<td>是否启用保护模式</td>
</tr>
<tr>
<td>eureka.server.renewalPercentThreshold</td>
<td>0.85</td>
<td>指定需要收到续租的阀值</td>
</tr>
<tr>
<td>eureka.server.renewalThresholdUpdateIntervalMs</td>
<td>15分钟</td>
<td>指定了renewalThresholdUpdate定时任务的调度频率</td>
</tr>
<tr>
<td>eureka.server.evictionIntervalTimerInMs</td>
<td>60 * 1000</td>
<td>指定了EvictionTask的调度频率，用于剔除过期的服务</td>
</tr>
<tr>
<td>eureka.server.useReadOnlyResponseCache</td>
<td>True</td>
<td>是否使用只读的response cache</td>
</tr>
<tr>
<td>eureka.server.responseCacheUpdateIntervalMs</td>
<td>30s</td>
<td>设置CacheUpdateTask的调度时间间隔</td>
</tr>
<tr>
<td>eureka.server.responseCacheAutoExpirationInSeconds</td>
<td></td>
<td>readWriteCacheMapr的expireAfterWrite，指定写入多常时间过期</td>
</tr>
<tr>
<td>eureka.server.peerEurekaNodesUpdateIntervalMs</td>
<td>10分钟</td>
<td>指定PeerUpdateTask的时间间隔</td>
</tr>
<tr>
<td>eureka.server.peerEurekaStatusRefreshTimeIntervalMs</td>
<td>30s</td>
<td>指定更新peer状态的时间隔</td>
</tr>
<tr>
<td>eureka.server.peerNodeConnectTimeoutMs</td>
<td>200ms</td>
<td>peer节点连接超时,单位ms</td>
</tr>
<tr>
<td>eureka.server.peerNodeReadTimeoutMs</td>
<td>200ms</td>
<td>peer节点读超时</td>
</tr>
<tr>
<td>eureka.server.peerNodeTotalConnections</td>
<td>1000</td>
<td>连接池最大连接数</td>
</tr>
</tbody>
</table>
<p>更多配置见于<strong>EurekaServerConfigBean</strong></p>
<p><strong>Client端</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>eureka.client.availabilityZones</td>
<td></td>
<td>告知client有哪些region及zone</td>
</tr>
<tr>
<td>eureka.client.region</td>
<td>us-east-1</td>
<td>指定该实例所在的region</td>
</tr>
<tr>
<td>eureka.client.registerWithEureka</td>
<td>true</td>
<td>是否向eureka server注册</td>
</tr>
<tr>
<td>eureka.client.preferSameZoneEureka</td>
<td>true</td>
<td>是否优先向同一zone的server注册或者拉取服务列表信息</td>
</tr>
<tr>
<td>eureka.client.filterOnlyUpInstances</td>
<td>true</td>
<td>是否过滤出InstanceStatus为UP的服务实例</td>
</tr>
<tr>
<td>eureka.client.serviceUrl</td>
<td></td>
<td>eureka server 的地址</td>
</tr>
<tr>
<td>eureka.client.eurekaServerReadTimeoutSeconds</td>
<td>8s</td>
<td>读超时时间</td>
</tr>
<tr>
<td>eureka.client.eurekaServerConnectTimeoutSeconds</td>
<td>5s</td>
<td>连接eureka server的超时时间</td>
</tr>
<tr>
<td>eureka.instance.metadataMap</td>
<td></td>
<td>eureka 实例的元数据信息</td>
</tr>
</tbody>
</table>
<p>更多配置见于<strong>EurekaClientConfigBean</strong></p>
<h1 id="eureka-rest端点">Eureka REST端点</h1>
<p><strong>appID</strong>: 应用名称</p>
<p><strong>instanceID</strong>：应用实例的id</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>Http action</th>
</tr>
</thead>
<tbody>
<tr>
<td>服务注册</td>
<td>POST /eureka/apps/<strong>appID</strong></td>
</tr>
<tr>
<td>服务下线</td>
<td>DELETE /eureka/apps/<strong>appID</strong>/<strong>instanceID</strong></td>
</tr>
<tr>
<td>发送心跳</td>
<td>PUT /eureka/apps/<strong>appID</strong>/<strong>instanceID</strong></td>
</tr>
<tr>
<td>查询所有appId的实例</td>
<td>GET /eureka/apps/<strong>appID</strong></td>
</tr>
<tr>
<td>查询 实例的信息</td>
<td>GET /eureka/apps/<strong>appID</strong>/<strong>instanceID</strong></td>
</tr>
</tbody>
</table>
<p>更多见https://github.com/Netflix/eureka/wiki/Eureka-REST-operations</p>
<p>调用endpoint一个例子：</p>
<figure data-type="image" tabindex="1"><img src="https://caijh.github.io//post-images/1572851163698.png" alt="" loading="lazy"></figure>
<h1 id="eureka-源码">Eureka 源码</h1>
<h2 id="server端"><strong>Server端</strong></h2>
<h3 id="启动过程"><strong>启动过程</strong></h3>
<ol>
<li>
<p>注解@EnableEurekaServer引入EurekaServerMarkerConfiguration，EurekaServerMarkerConfiguration中生成一个名为Maker的Bean，其作用是EurekaServerAutoConfiguration生效</p>
</li>
<li>
<p>EurekaServerAutoConfiguration中又引入了EurekaServerInitializerConfiguration配置，EurekaServerInitializerConfiguration中的start方法启动了eureka server</p>
<pre><code class="language-java">@Override
public void start() {
   new Thread(new Runnable() {
      @Override
      public void run() {
         try {
            // TODO: is this class even needed now?
            eurekaServerBootstrap.contextInitialized(
                  EurekaServerInitializerConfiguration.this.servletContext);
            log.info(&quot;Started Eureka Server&quot;);

            publish(new EurekaRegistryAvailableEvent(getEurekaServerConfig()));
            EurekaServerInitializerConfiguration.this.running = true;
            publish(new EurekaServerStartedEvent(getEurekaServerConfig()));
         }
         catch (Exception ex) {
            // Help!
            log.error(&quot;Could not initialize Eureka servlet context&quot;, ex);
         }
      }
   }).start();
}
</code></pre>
</li>
<li>
<p>eurekaServerBootstrap.contextInitialized中完成初始化eureka环境变量，eureka上下文</p>
<pre><code class="language-java">public void contextInitialized(ServletContext context) {
   try {
      initEurekaEnvironment();
      initEurekaServerContext();

      context.setAttribute(EurekaServerContext.class.getName(), this.serverContext);
   }
   catch (Throwable e) {
      log.error(&quot;Cannot bootstrap eureka server :&quot;, e);
      throw new RuntimeException(&quot;Cannot bootstrap eureka server :&quot;, e);
   }
}

</code></pre>
</li>
<li>
<p>initEurekaServerContext启动Eureka Server</p>
<pre><code class="language-java">protected void initEurekaServerContext() throws Exception {
   // For backward compatibility
   JsonXStream.getInstance().registerConverter(new V1AwareInstanceInfoConverter(),
         XStream.PRIORITY_VERY_HIGH);
   XmlXStream.getInstance().registerConverter(new V1AwareInstanceInfoConverter(),
         XStream.PRIORITY_VERY_HIGH);

   if (isAws(this.applicationInfoManager.getInfo())) {
      this.awsBinder = new AwsBinderDelegate(this.eurekaServerConfig,
            this.eurekaClientConfig, this.registry, this.applicationInfoManager);
      this.awsBinder.start();
   }

   EurekaServerContextHolder.initialize(this.serverContext);

   log.info(&quot;Initialized server context&quot;);

   // Copy registry from neighboring eureka node
   int registryCount = this.registry.syncUp();  // 拉取服务信息
   this.registry.openForTraffic(this.applicationInfoManager, registryCount);

   // Register all monitoring statistics.
   EurekaMonitors.registerAllStats();
}
</code></pre>
</li>
</ol>
<h3 id="剔除服务"><strong>剔除服务</strong></h3>
<p>server端剔除服务实例是由一个位于AbstractInstanceRegistry中名为evictionTimer的Timer定时器来执行EvictionTask任务。</p>
<pre><code class="language-java">public void evict(long additionalLeaseMs) {
    logger.debug(&quot;Running the evict task&quot;);

    if (!isLeaseExpirationEnabled()) {
        logger.debug(&quot;DS: lease expiration is currently disabled.&quot;);
        return;
    }

    // We collect first all expired items, to evict them in random order. For large eviction sets,
    // if we do not that, we might wipe out whole apps before self preservation kicks in. By randomizing it,
    // the impact should be evenly distributed across all applications.
    List&lt;Lease&lt;InstanceInfo&gt;&gt; expiredLeases = new ArrayList&lt;&gt;();
    for (Entry&lt;String, Map&lt;String, Lease&lt;InstanceInfo&gt;&gt;&gt; groupEntry : registry.entrySet()) {
        Map&lt;String, Lease&lt;InstanceInfo&gt;&gt; leaseMap = groupEntry.getValue();
        if (leaseMap != null) {
            for (Entry&lt;String, Lease&lt;InstanceInfo&gt;&gt; leaseEntry : leaseMap.entrySet()) {
                Lease&lt;InstanceInfo&gt; lease = leaseEntry.getValue();
                if (lease.isExpired(additionalLeaseMs) &amp;&amp; lease.getHolder() != null) {
                    expiredLeases.add(lease);
                }
            }
        }
    }

    // To compensate for GC pauses or drifting local time, we need to use current registry size as a base for
    // triggering self-preservation. Without that we would wipe out full registry.
    int registrySize = (int) getLocalRegistrySize();
    int registrySizeThreshold = (int) (registrySize * serverConfig.getRenewalPercentThreshold());
    int evictionLimit = registrySize - registrySizeThreshold;

    int toEvict = Math.min(expiredLeases.size(), evictionLimit);
    if (toEvict &gt; 0) {
        logger.info(&quot;Evicting {} items (expired={}, evictionLimit={})&quot;, toEvict, expiredLeases.size(), evictionLimit);

        Random random = new Random(System.currentTimeMillis());
        for (int i = 0; i &lt; toEvict; i++) {
            // Pick a random item (Knuth shuffle algorithm)
            int next = i + random.nextInt(expiredLeases.size() - i);
            Collections.swap(expiredLeases, i, next);
            Lease&lt;InstanceInfo&gt; lease = expiredLeases.get(i);

            String appName = lease.getHolder().getAppName();
            String id = lease.getHolder().getId();
            EXPIRED.increment();
            logger.warn(&quot;DS: Registry: expired lease for {}/{}&quot;, appName, id);
            internalCancel(appName, id, false); // 注销服务实例
        }
    }
}
</code></pre>
<h2 id="client端">Client端</h2>
<h3 id="服务注册">服务注册</h3>
<ol>
<li>
<p>从EnableDiscoveryClient注解开始</p>
<p>由注解@EnableDiscoveryClient引入AutoServiceRegistrationConfiguration，AutoServiceRegistrationConfiguration开启通过配置spring.cloud.service-registry.auto-registration.enabled，并创建Marker Bean实例</p>
</li>
<li>
<p>EurekaClientAutoConfiguration类创建EurekaAutoServiceRegistration Bean</p>
<pre><code class="language-java">@Bean
@ConditionalOnBean(AutoServiceRegistrationProperties.class)
@ConditionalOnProperty(value = &quot;spring.cloud.service-registry.auto-registration.enabled&quot;, matchIfMissing = true)
public EurekaAutoServiceRegistration eurekaAutoServiceRegistration(
      ApplicationContext context, EurekaServiceRegistry registry,
      EurekaRegistration registration) {
   return new EurekaAutoServiceRegistration(context, registry, registration);
}

</code></pre>
<p>EurekaAutoServiceRegistration是实现了<strong>SmartLifecycle</strong>接口，start方法完成初始化</p>
<pre><code class="language-java">@Override
public void start() {
   // only set the port if the nonSecurePort or securePort is 0 and this.port != 0
   if (this.port.get() != 0) {
      if (this.registration.getNonSecurePort() == 0) {
         this.registration.setNonSecurePort(this.port.get());
      }

      if (this.registration.getSecurePort() == 0 &amp;&amp; this.registration.isSecure()) {
         this.registration.setSecurePort(this.port.get());
      }
   }

   // only initialize if nonSecurePort is greater than 0 and it isn't already running
   // because of containerPortInitializer below
   if (!this.running.get() &amp;&amp; this.registration.getNonSecurePort() &gt; 0) {

      this.serviceRegistry.register(this.registration); // 实际上没有注册，只是get信息

      this.context.publishEvent(new InstanceRegisteredEvent&lt;&gt;(this,
            this.registration.getInstanceConfig()));
      this.running.set(true);
   }
}

</code></pre>
</li>
<li>
<p>CloudEurekaClient的初始化</p>
<pre><code class="language-java">@Bean(destroyMethod = &quot;shutdown&quot;)
@ConditionalOnMissingBean(value = EurekaClient.class, search = SearchStrategy.CURRENT)
public EurekaClient eurekaClient(ApplicationInfoManager manager,
      EurekaClientConfig config) {
   return new CloudEurekaClient(manager, config, this.optionalArgs,
         this.context);
}

</code></pre>
<pre><code class="language-java">@Inject
DiscoveryClient(ApplicationInfoManager applicationInfoManager, EurekaClientConfig config, AbstractDiscoveryClientOptionalArgs args,
                Provider&lt;BackupRegistry&gt; backupRegistryProvider, EndpointRandomizer endpointRandomizer) {
    if (args != null) {
        this.healthCheckHandlerProvider = args.healthCheckHandlerProvider;
        this.healthCheckCallbackProvider = args.healthCheckCallbackProvider;
        this.eventListeners.addAll(args.getEventListeners());
        this.preRegistrationHandler = args.preRegistrationHandler;
    } else {
        this.healthCheckCallbackProvider = null;
        this.healthCheckHandlerProvider = null;
        this.preRegistrationHandler = null;
    }
    
    this.applicationInfoManager = applicationInfoManager;
    InstanceInfo myInfo = applicationInfoManager.getInfo();

    clientConfig = config;
    staticClientConfig = clientConfig;
    transportConfig = config.getTransportConfig();
    instanceInfo = myInfo;
    if (myInfo != null) {
        appPathIdentifier = instanceInfo.getAppName() + &quot;/&quot; + instanceInfo.getId();
    } else {
        logger.warn(&quot;Setting instanceInfo to a passed in null value&quot;);
    }

    this.backupRegistryProvider = backupRegistryProvider;
    this.endpointRandomizer = endpointRandomizer;
    this.urlRandomizer = new EndpointUtils.InstanceInfoBasedUrlRandomizer(instanceInfo);
    localRegionApps.set(new Applications());

    fetchRegistryGeneration = new AtomicLong(0);

    remoteRegionsToFetch = new AtomicReference&lt;String&gt;(clientConfig.fetchRegistryForRemoteRegions());
    remoteRegionsRef = new AtomicReference&lt;&gt;(remoteRegionsToFetch.get() == null ? null : remoteRegionsToFetch.get().split(&quot;,&quot;));

    if (config.shouldFetchRegistry()) {
        this.registryStalenessMonitor = new ThresholdLevelsMetric(this, METRIC_REGISTRY_PREFIX + &quot;lastUpdateSec_&quot;, new long[]{15L, 30L, 60L, 120L, 240L, 480L});
    } else {
        this.registryStalenessMonitor = ThresholdLevelsMetric.NO_OP_METRIC;
    }

    if (config.shouldRegisterWithEureka()) {
        this.heartbeatStalenessMonitor = new ThresholdLevelsMetric(this, METRIC_REGISTRATION_PREFIX + &quot;lastHeartbeatSec_&quot;, new long[]{15L, 30L, 60L, 120L, 240L, 480L});
    } else {
        this.heartbeatStalenessMonitor = ThresholdLevelsMetric.NO_OP_METRIC;
    }

    logger.info(&quot;Initializing Eureka in region {}&quot;, clientConfig.getRegion());

    if (!config.shouldRegisterWithEureka() &amp;&amp; !config.shouldFetchRegistry()) {
        logger.info(&quot;Client configured to neither register nor query for data.&quot;);
        scheduler = null;
        heartbeatExecutor = null;
        cacheRefreshExecutor = null;
        eurekaTransport = null;
        instanceRegionChecker = new InstanceRegionChecker(new PropertyBasedAzToRegionMapper(config), clientConfig.getRegion());

        // This is a bit of hack to allow for existing code using DiscoveryManager.getInstance()
        // to work with DI'd DiscoveryClient
        DiscoveryManager.getInstance().setDiscoveryClient(this);
        DiscoveryManager.getInstance().setEurekaClientConfig(config);

        initTimestampMs = System.currentTimeMillis();
        logger.info(&quot;Discovery Client initialized at timestamp {} with initial instances count: {}&quot;,
                initTimestampMs, this.getApplications().size());

        return;  // no need to setup up an network tasks and we are done
    }

    try {
        // default size of 2 - 1 each for heartbeat and cacheRefresh
        scheduler = Executors.newScheduledThreadPool(2,
                new ThreadFactoryBuilder()
                        .setNameFormat(&quot;DiscoveryClient-%d&quot;)
                        .setDaemon(true)
                        .build());

        heartbeatExecutor = new ThreadPoolExecutor(
                1, clientConfig.getHeartbeatExecutorThreadPoolSize(), 0, TimeUnit.SECONDS,
                new SynchronousQueue&lt;Runnable&gt;(),
                new ThreadFactoryBuilder()
                        .setNameFormat(&quot;DiscoveryClient-HeartbeatExecutor-%d&quot;)
                        .setDaemon(true)
                        .build()
        );  // use direct handoff

        cacheRefreshExecutor = new ThreadPoolExecutor(
                1, clientConfig.getCacheRefreshExecutorThreadPoolSize(), 0, TimeUnit.SECONDS,
                new SynchronousQueue&lt;Runnable&gt;(),
                new ThreadFactoryBuilder()
                        .setNameFormat(&quot;DiscoveryClient-CacheRefreshExecutor-%d&quot;)
                        .setDaemon(true)
                        .build()
        );  // use direct handoff

        eurekaTransport = new EurekaTransport();
        scheduleServerEndpointTask(eurekaTransport, args); // 准备访问eureka server的客户端

        AzToRegionMapper azToRegionMapper;
        if (clientConfig.shouldUseDnsForFetchingServiceUrls()) {
            azToRegionMapper = new DNSBasedAzToRegionMapper(clientConfig);
        } else {
            azToRegionMapper = new PropertyBasedAzToRegionMapper(clientConfig);
        }
        if (null != remoteRegionsToFetch.get()) {
            azToRegionMapper.setRegionsToFetch(remoteRegionsToFetch.get().split(&quot;,&quot;));
        }
        instanceRegionChecker = new InstanceRegionChecker(azToRegionMapper, clientConfig.getRegion());
    } catch (Throwable e) {
        throw new RuntimeException(&quot;Failed to initialize DiscoveryClient!&quot;, e);
    }

    if (clientConfig.shouldFetchRegistry() &amp;&amp; !fetchRegistry(false)) { // 获取已经注册的服务
        fetchRegistryFromBackup();
    }

    // call and execute the pre registration handler before all background tasks (inc registration) is started
    if (this.preRegistrationHandler != null) {
        this.preRegistrationHandler.beforeRegistration();
    }

    if (clientConfig.shouldRegisterWithEureka() &amp;&amp; clientConfig.shouldEnforceRegistrationAtInit()) {
        try {
            if (!register() ) { // 如果是否在初始始化时向Eureka Server注册
                throw new IllegalStateException(&quot;Registration error at startup. Invalid server response.&quot;);
            }
        } catch (Throwable th) {
            logger.error(&quot;Registration error at startup: {}&quot;, th.getMessage());
            throw new IllegalStateException(th);
        }
    }

    // finally, init the schedule tasks (e.g. cluster resolvers, heartbeat, instanceInfo replicator, fetch
    initScheduledTasks();  // 初始化定时任务

    try {
        Monitors.registerObject(this);
    } catch (Throwable e) {
        logger.warn(&quot;Cannot register timers&quot;, e);
    }

    // This is a bit of hack to allow for existing code using DiscoveryManager.getInstance()
    // to work with DI'd DiscoveryClient
    DiscoveryManager.getInstance().setDiscoveryClient(this);
    DiscoveryManager.getInstance().setEurekaClientConfig(config);

    initTimestampMs = System.currentTimeMillis();
    logger.info(&quot;Discovery Client initialized at timestamp {} with initial instances count: {}&quot;,
            initTimestampMs, this.getApplications().size());
}
</code></pre>
</li>
<li>
<p>向eureka server注册是由instanceInfoReplicator的start方法，这时默认延迟40s注册</p>
<pre><code class="language-java">/**
 * Register with the eureka service by making the appropriate REST call.
 */
boolean register() throws Throwable {
    logger.info(PREFIX + &quot;{}: registering service...&quot;, appPathIdentifier);
    EurekaHttpResponse&lt;Void&gt; httpResponse;
    try {
        httpResponse = eurekaTransport.registrationClient.register(instanceInfo);
    } catch (Exception e) {
        logger.warn(PREFIX + &quot;{} - registration failed {}&quot;, appPathIdentifier, e.getMessage(), e);
        throw e;
    }
    if (logger.isInfoEnabled()) {
        logger.info(PREFIX + &quot;{} - registration status: {}&quot;, appPathIdentifier, httpResponse.getStatusCode());
    }
    return httpResponse.getStatusCode() == Status.NO_CONTENT.getStatusCode();
}
</code></pre>
<pre><code>@Override
public EurekaHttpResponse&lt;Void&gt; register(InstanceInfo info) {
    String urlPath = &quot;apps/&quot; + info.getAppName();
    ClientResponse response = null;
    try {
        Builder resourceBuilder = jerseyClient.resource(serviceUrl).path(urlPath).getRequestBuilder();
        addExtraHeaders(resourceBuilder);
        response = resourceBuilder
                .header(&quot;Accept-Encoding&quot;, &quot;gzip&quot;)
                .type(MediaType.APPLICATION_JSON_TYPE)
                .accept(MediaType.APPLICATION_JSON)
                .post(ClientResponse.class, info);
        return anEurekaHttpResponse(response.getStatus()).headers(headersOf(response)).build();
    } finally {
        if (logger.isDebugEnabled()) {
            logger.debug(&quot;Jersey HTTP POST {}/{} with instance {}; statusCode={}&quot;, serviceUrl, urlPath, info.getId(),
                    response == null ? &quot;N/A&quot; : response.getStatus());
        }
        if (response != null) {
            response.close();
        }
    }
}
</code></pre>
</li>
</ol>
<h3 id="服务续约">服务续约</h3>
<p>心跳检测执行器heartbeatExecutor会定时执行线程HeartbeatThread，会执行renew向eureka server续租</p>
<pre><code class="language-java">/**
 * Renew with the eureka service by making the appropriate REST call
 */
boolean renew() {
    EurekaHttpResponse&lt;InstanceInfo&gt; httpResponse;
    try {
        httpResponse = eurekaTransport.registrationClient.sendHeartBeat(instanceInfo.getAppName(), instanceInfo.getId(), instanceInfo, null);
        logger.debug(PREFIX + &quot;{} - Heartbeat status: {}&quot;, appPathIdentifier, httpResponse.getStatusCode());
        if (httpResponse.getStatusCode() == Status.NOT_FOUND.getStatusCode()) {
            REREGISTER_COUNTER.increment();
            logger.info(PREFIX + &quot;{} - Re-registering apps/{}&quot;, appPathIdentifier, instanceInfo.getAppName());
            long timestamp = instanceInfo.setIsDirtyWithTime();
            boolean success = register();
            if (success) {
                instanceInfo.unsetIsDirty(timestamp);
            }
            return success;
        }
        return httpResponse.getStatusCode() == Status.OK.getStatusCode();
    } catch (Throwable e) {
        logger.error(PREFIX + &quot;{} - was unable to send heartbeat!&quot;, appPathIdentifier, e);
        return false;
    }
}
</code></pre>
<h3 id="服务下线">服务下线</h3>
<pre><code class="language-java">@PreDestroy
@Override
public synchronized void shutdown() {
    if (isShutdown.compareAndSet(false, true)) {
        logger.info(&quot;Shutting down DiscoveryClient ...&quot;);

        if (statusChangeListener != null &amp;&amp; applicationInfoManager != null) {
            applicationInfoManager.unregisterStatusChangeListener(statusChangeListener.getId());
        }

        cancelScheduledTasks();

        // If APPINFO was registered
        if (applicationInfoManager != null
                &amp;&amp; clientConfig.shouldRegisterWithEureka()
                &amp;&amp; clientConfig.shouldUnregisterOnShutdown()) {
            applicationInfoManager.setInstanceStatus(InstanceStatus.DOWN);
            unregister();
        }

        if (eurekaTransport != null) {
            eurekaTransport.shutdown();
        }

        heartbeatStalenessMonitor.shutdown();
        registryStalenessMonitor.shutdown();

        logger.info(&quot;Completed shut down of DiscoveryClient&quot;);
    }
}
</code></pre>
<h3 id="更新服务列表">更新服务列表</h3>
<p>默认每隔30s从服务端GET一次增量版本信息，然后和本地比较并合并，保证本地能获取到其他节点最新注册信息。</p>
<p>TimedSupervisorTask每隔30s执行CacheRefreshThread线程</p>
<pre><code class="language-java">class CacheRefreshThread implements Runnable {
    public void run() {
        refreshRegistry();
    }
}

@VisibleForTesting
void refreshRegistry() {
    try {
        boolean isFetchingRemoteRegionRegistries = isFetchingRemoteRegionRegistries();

        boolean remoteRegionsModified = false;
        // This makes sure that a dynamic change to remote regions to fetch is honored.
        String latestRemoteRegions = clientConfig.fetchRegistryForRemoteRegions();
        if (null != latestRemoteRegions) {
            String currentRemoteRegions = remoteRegionsToFetch.get();
            if (!latestRemoteRegions.equals(currentRemoteRegions)) {
                // Both remoteRegionsToFetch and AzToRegionMapper.regionsToFetch need to be in sync
                synchronized (instanceRegionChecker.getAzToRegionMapper()) {
                    if (remoteRegionsToFetch.compareAndSet(currentRemoteRegions, latestRemoteRegions)) {
                        String[] remoteRegions = latestRemoteRegions.split(&quot;,&quot;);
                        remoteRegionsRef.set(remoteRegions);
                        instanceRegionChecker.getAzToRegionMapper().setRegionsToFetch(remoteRegions);
                        remoteRegionsModified = true;
                    } else {
                        logger.info(&quot;Remote regions to fetch modified concurrently,&quot; +
                                &quot; ignoring change from {} to {}&quot;, currentRemoteRegions, latestRemoteRegions);
                    }
                }
            } else {
                // Just refresh mapping to reflect any DNS/Property change
                instanceRegionChecker.getAzToRegionMapper().refreshMapping();
            }
        }

        boolean success = fetchRegistry(remoteRegionsModified);
        if (success) {
            registrySize = localRegionApps.get().size();
            lastSuccessfulRegistryFetchTimestamp = System.currentTimeMillis();
        }

        if (logger.isDebugEnabled()) {
            StringBuilder allAppsHashCodes = new StringBuilder();
            allAppsHashCodes.append(&quot;Local region apps hashcode: &quot;);
            allAppsHashCodes.append(localRegionApps.get().getAppsHashCode());
            allAppsHashCodes.append(&quot;, is fetching remote regions? &quot;);
            allAppsHashCodes.append(isFetchingRemoteRegionRegistries);
            for (Map.Entry&lt;String, Applications&gt; entry : remoteRegionVsApps.entrySet()) {
                allAppsHashCodes.append(&quot;, Remote region: &quot;);
                allAppsHashCodes.append(entry.getKey());
                allAppsHashCodes.append(&quot; , apps hashcode: &quot;);
                allAppsHashCodes.append(entry.getValue().getAppsHashCode());
            }
            logger.debug(&quot;Completed cache refresh task for discovery. All Apps hash code is {} &quot;,
                    allAppsHashCodes);
        }
    } catch (Throwable e) {
        logger.error(&quot;Cannot fetch registry from server&quot;, e);
    }
}
</code></pre>
<h1 id="eureka-性能">Eureka 性能</h1>
<p>https://www.toutiao.com/i6661231047201522183/</p>
<p>http://springcloud.cn/view/31</p>
<p>参考以上两个</p>
<p>从其测试的来看一个Eureka最多支持多少个eureka实例是由操作系统、CPU线程、内存，应用容器连接数等限制的。</p>
<h1 id="问题">问题</h1>
<ol>
<li>一个Eureka Server最支持多少个eureka client?  要看具体的环境操作系统、CPU线程、内存，应用容器连接数等限制的</li>
<li>eureka client的zone与eureka server的zone不一样时，eureka client会向eureka server 注册吗？</li>
<li>。。。</li>
</ol>
<h1 id="eureka-容器化及k8s部署">Eureka 容器化及k8s部署</h1>
<pre><code>---
apiVersion: v1
kind: ConfigMap
metadata:
  name: eureka-cm
  namespace: hjmos
data:
  # if you want to deploy n instances of eureka cluster,
  # you should set eureka_service_address: http://eureka-0.eureka:8761/eureka,...,http://eureka-(n-1).eureka:8761/eureka
  eureka_service_address: http://eureka-0.eureka:8761/eureka,http://eureka-1.eureka:8761/eureka
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: eureka
  namespace: hjmos
spec:
  serviceName: 'eureka'
  replicas: 2
  selector:
    matchLabels:
      app: eureka # 符合目标的Pod有此标签
  template:
    metadata:
      labels:
        app: eureka  # Pod副本的标签
    spec:
      containers:
        - name: service-registry
          image: 10.38.2.12:1000/library/service-registry:1.0.0
          ports:
            - containerPort: 8761
          args:
            - --spring.profiles.active=dev
            - --management.server.port=8081
            - --management.endpoints.web.exposure.include=*
            - --management.endpoint.health.show-details=always
            - --eureka.instance.health-check-url-path=/actuator/health
          env:
            - name: EUREKA_SERVER_ADDRESS
              valueFrom:
                configMapKeyRef:
                  name: eureka-cm
                  key: eureka_service_address

</code></pre>
<p>eureka-cm 存放eureka server集群信息</p>
<pre><code>---
apiVersion: v1
kind: Service
metadata:
  name: eureka
  namespace: hjmos
  labels:
    app: eureka
spec:
  clusterIP: None
  ports:
    - port: 8761
      name: eureka
  selector:
    app: eureka

</code></pre>
<pre><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: service-registry-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
spec:
  rules:
    - http:
        paths:
          - path: /service-registry(/|$)(.*)
            backend:
              serviceName: eureka
              servicePort: 8761
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: eureka-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
spec:
  rules:
    - http:
        paths:
          - path: /eureka
            backend:
              serviceName: eureka
              servicePort: 8761

</code></pre>
<p>外部访问http://ip:port/service-registry, 最好的形式是ingress使用host</p>
<p>Eureka Client有三种形式可以注册到eureka 集群</p>
<ol>
<li>
<p>eureka client和eureka server如果部署在kubernetes的同一个namespace。此时可以使用名为eureka-cm的ConfigMap的eureka_service_address配置数据设置</p>
</li>
<li>
<p>eureka client和eureka server如果部署在同一个kubernetes集群中。此时可以使用headless service的域名地址，这里eureka默认部署在kubernetes的default的命名空间中</p>
<pre><code>eureka:
  client:
    serviceUrl:
      defaultZone: http://eureka-0.eureka.default.svc.cluster.local:8761/eureka/,http://eureka-1.eureka.default.svc.cluster.local:8761/eureka/,http://eureka-2.eureka.default.svc.cluster.local:8761/eureka/
</code></pre>
</li>
<li>
<p>http://ip:port/service-registry</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes - 部署rabbitmq集群]]></title>
        <id>https://caijh.github.io/post/kubernetes-bu-shu-rabbitmq-ji-qun</id>
        <link href="https://caijh.github.io/post/kubernetes-bu-shu-rabbitmq-ji-qun">
        </link>
        <updated>2019-08-30T01:05:56.000Z</updated>
        <summary type="html"><![CDATA[<hr>
<p>author: caijunhui<br>
date: 2019-08-29</p>
<hr>
<p>本文介绍如何在k8s环境下部署rabbitmq</p>
<p>镜像使用rabbitmq:3.7-management-alpine，从3.7.0开始rabbitmq使用了peer discovery， 不使用autoscale。</p>
]]></summary>
        <content type="html"><![CDATA[<hr>
<p>author: caijunhui<br>
date: 2019-08-29</p>
<hr>
<p>本文介绍如何在k8s环境下部署rabbitmq</p>
<p>镜像使用rabbitmq:3.7-management-alpine，从3.7.0开始rabbitmq使用了peer discovery， 不使用autoscale。</p>
<!-- more -->
<ol>
<li>
<p>创建rabbitmq的namespce</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: rabbitmq
</code></pre>
</li>
<li>
<p>rdbc</p>
<pre><code class="language-yaml">---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rabbitmq 
  namespace: rabbitmq 
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: endpoint-reader
  namespace: rabbitmq 
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;endpoints&quot;]
  verbs: [&quot;get&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: endpoint-reader
  namespace: rabbitmq
subjects:
- kind: ServiceAccount
  name: rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: endpoint-reader
</code></pre>
</li>
<li>
<p>rabbitmq数据持久化</p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rabbitmq-pvc
  namespace: rabbitmq
spec:
  accessModes:
    - ReadWriteMany
  resources:  
    requests:
      storage: 5Gi
  storageClassName: managed-nfs-storage
</code></pre>
</li>
<li>
<p>Deploy</p>
<pre><code class="language-yaml"># NodePort Servce，以便通过集群节点访问rabbitmq
kind: Service
apiVersion: v1
metadata:
  namespace: rabbitmq
  name: rabbitmq-svc
  labels:
    app: rabbitmq
    type: LoadBalancer  
spec:
  type: NodePort
  ports:
   - name: rabbitmq-mgmt-port
     protocol: TCP
     port: 15672
     targetPort: 15672
     nodePort: 31672
   - name: rabbitmq-amqp-port
     protocol: TCP
     port: 5672
     targetPort: 5672
     nodePort: 30672
  selector:
    app: rabbitmq
    
---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
  namespace: rabbitmq
  labels:
    app: rabbitmq
spec:
  clusterIP: None
  #用Headless Service去做Pod的hostname访问，需要等Pod和Service都启动后才能访问，而readiness探针还没等DNS正常就去探查服务是否可用，所以才会误认为服务不可达，最终无法启动Pod。解决办法是给Headless Service设置publishNotReadyAddresses: true
  publishNotReadyAddresses: true
  ports:
  - port: 5672
    protocol: TCP
    name: amqp
  - port: 4369
    name: epmd
  - port: 15672
    protocol: TCP
    name: http
  selector:
    app: rabbitmq
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-cm
  namespace: rabbitmq
data:
  enabled_plugins: |
      [rabbitmq_management,rabbitmq_peer_discovery_k8s].
  rabbitmq.conf: |
      ## Cluster formation. See https://www.rabbitmq.com/cluster-formation.html to learn more.
      cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
      cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
      # 必须设置service_name，否则Pod无法正常启动，这里设置后可以不设置statefulset下env中的K8S_SERVICE_NAME变量
      cluster_formation.k8s.service_name = rabbitmq
      # 必须设置hostname_suffix，否则节点不能成为集群
      cluster_formation.k8s.hostname_suffix = .rabbitmq.rabbitmq.svc.cluster.local
      ## Should RabbitMQ node name be computed from the pod's hostname or IP address?
      ## IP addresses are not stable, so using [stable] hostnames is recommended when possible.
      ## Set to &quot;hostname&quot; to use pod hostnames.
      ## When this value is changed, so should the variable used to set the RABBITMQ_NODENAME
      ## environment variable.
      ## hostname or ip
      cluster_formation.k8s.address_type = hostname
      ## How often should node cleanup checks run?
      cluster_formation.node_cleanup.interval = 30
      cluster_formation.randomized_startup_delay_range.min = 0
      cluster_formation.randomized_startup_delay_range.max = 2
      ## Set to false if automatic removal of unknown/absent nodes
      ## is desired. This can be dangerous, see
      ##  * https://www.rabbitmq.com/cluster-formation.html#node-health-checks-and-cleanup
      ##  * https://groups.google.com/forum/#!msg/rabbitmq-users/wuOfzEywHXo/k8z_HWIkBgAJ
      cluster_formation.node_cleanup.only_log_warning = true
      cluster_partition_handling = autoheal
      ## See https://www.rabbitmq.com/ha.html#master-migration-data-locality
      queue_master_locator=min-masters
      ## See https://www.rabbitmq.com/access-control.html#loopback-users
      loopback_users.guest = false
   
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: rabbitmq
  namespace: rabbitmq
spec:
  serviceName: rabbitmq
  selector:
    matchLabels:
      app: rabbitmq
  replicas: 3
  template:
    metadata:
      labels:
        app: rabbitmq
      annotations:
              scheduler.alpha.kubernetes.io/affinity: &gt;
                  {
                    &quot;podAntiAffinity&quot;: {
                      &quot;requiredDuringSchedulingIgnoredDuringExecution&quot;: [{
                        &quot;labelSelector&quot;: {
                          &quot;matchExpressions&quot;: [{
                            &quot;key&quot;: &quot;app&quot;,
                            &quot;operator&quot;: &quot;In&quot;,
                            &quot;values&quot;: [&quot;rabbitmq&quot;]
                          }]
                        },
                        &quot;topologyKey&quot;: &quot;kubernetes.io/hostname&quot;
                      }]
                    }
                  }
    spec:
      serviceAccountName: rabbitmq
      terminationGracePeriodSeconds: 10
      containers:        
      - name: rabbitmq-k8s
        image: rabbitmq:3.7-management-alpine
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: config-volume
            mountPath: /etc/rabbitmq
          - name: rabbitmq-data
          	mountPath: /var/lib/rabbitmq/data
        ports:
          - name: http
            protocol: TCP
            containerPort: 15672
          - name: amqp
            protocol: TCP
            containerPort: 5672
          - containerPort: 4369
          - containerPort: 25672
        livenessProbe:
          exec:
            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]
          initialDelaySeconds: 60
          # See https://www.rabbitmq.com/monitoring.html for monitoring frequency recommendations.
          periodSeconds: 60
          timeoutSeconds: 15
        readinessProbe:
          exec:
            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]
          initialDelaySeconds: 20
          periodSeconds: 60
          timeoutSeconds: 10
        env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: RABBITMQ_USE_LONGNAME
            value: &quot;true&quot;
          # - name: K8S_SERVICE_NAME
          #  value: &quot;rabbitmq&quot;
          # See a note on cluster_formation.k8s.address_type in the config file section
          - name: RABBITMQ_NODENAME
            value: &quot;rabbit@$(MY_POD_NAME).rabbitmq.rabbitmq.svc.cluster.local&quot;
          - name: RABBITMQ_ERLANG_COOKIE
            value: &quot;Zk93VStwK0g3ZE5KNkxlT0lRQ0V2S3BTNXBockk0UU9QeVNrSXdRSGM0RT0K&quot;
          #  valueFrom:
          #    secretKeyRef:
          #      name: erlang.cookie
          #      key: erlang.cookie 
      volumes:
        - name: config-volume
          configMap:
            name: rabbitmq-cm
            items:
            - key: rabbitmq.conf
              path: rabbitmq.conf
            - key: enabled_plugins
              path: enabled_plugins
        - name: rabbitmq-data
        	persistentVolumeClaim:
            claimName: rabbitmq-pvc
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux - 安装nfs]]></title>
        <id>https://caijh.github.io/post/linux-an-zhuang-nfs</id>
        <link href="https://caijh.github.io/post/linux-an-zhuang-nfs">
        </link>
        <updated>2019-08-28T08:50:56.000Z</updated>
        <summary type="html"><![CDATA[<p>想要k8s中使用nfs来动态提供PersistentVolume，so查下怎么搭建nfs server.<br>
环境：<br>
centos 7</p>
]]></summary>
        <content type="html"><![CDATA[<p>想要k8s中使用nfs来动态提供PersistentVolume，so查下怎么搭建nfs server.<br>
环境：<br>
centos 7</p>
<!-- more -->
<ol>
<li>安装nfs-utils</li>
</ol>
<pre><code>yum install -y nfs-utils
</code></pre>
<ol start="2">
<li>修改配置</li>
</ol>
<pre><code>mkdir /nfs
chmod 777 /nfs
vim /etc/exports
/nfs   192.168.0.0/24(rw,sync,no_root_squash,no_all_squash)
</code></pre>
<p>常用选项：<br>
ro：客户端挂载后，其权限为只读，默认选项；<br>
rw:读写权限；<br>
sync：同时将数据写入到内存与硬盘中；<br>
async：异步，优先将数据保存到内存，然后再写入硬盘；<br>
Secure：要求请求源的端口小于1024<br>
用户映射：<br>
root_squash:当NFS客户端使用root用户访问时，映射到NFS服务器的匿名用户；<br>
no_root_squash:当NFS客户端使用root用户访问时，映射到NFS服务器的root用户；<br>
all_squash:全部用户都映射为服务器端的匿名用户；<br>
anonuid=UID：将客户端登录用户映射为此处指定的用户uid；<br>
anongid=GID：将客户端登录用户映射为此处指定的用户gid<br>
3. 启动并设置开机启动</p>
<pre><code>sudo systemctl enable nfs
sudo systemctl start nfs
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s - 安装kube-dns插件]]></title>
        <id>https://caijh.github.io/post/k8s-an-zhuang-kube-dns-cha-jian</id>
        <link href="https://caijh.github.io/post/k8s-an-zhuang-kube-dns-cha-jian">
        </link>
        <updated>2019-08-09T07:11:13.000Z</updated>
        <content type="html"><![CDATA[<h1 id="kubernetes-安装kube-dns插件">Kubernetes - 安装kube-dns插件</h1>
<p>k8s 版本: 1.5.2</p>
<ol>
<li>下载yaml文件，根据实际情况修改文件</li>
<li>kubectl creata -f yaml</li>
<li>重启node</li>
<li>测试</li>
</ol>
<h2 id="下载所需的文件">下载所需的文件</h2>
<p>主要有yaml文件和yaml文件要使用到的镜像， 镜像因为gfw的问题，要拉到本地再上传到服务器，或者使用docker 私有这仓库。</p>
<p>skydns-rc.yaml文件如下, 修改domain参数，增加<strong>kube-master-url</strong>参数</p>
<pre><code># Copyright 2016 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*
# Should keep target in cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
# in sync with this file.

# __MACHINE_GENERATED_WARNING__

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: &quot;true&quot;
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 0
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        scheduler.alpha.kubernetes.io/tolerations: '[{&quot;key&quot;:&quot;CriticalAddonsOnly&quot;, &quot;operator&quot;:&quot;Exists&quot;}]'
    spec:
      containers:
      - name: kubedns
        image: gcr.io/google_containers/kubedns-amd64:1.9
        imagePullPolicy: IfNotPresent
        resources:
          # TODO: Set memory limits when we've profiled the container for large
          # clusters, then set request = limit to keep this container in
          # guaranteed class. Currently, this container falls into the
          # &quot;burstable&quot; category so the kubelet doesn't backoff from restarting it.
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        livenessProbe:
          httpGet:
            path: /healthz-kubedns
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /readiness
            port: 8081
            scheme: HTTP
          # we poll on pod startup for the Kubernetes master service and
          # only setup the /readiness HTTP server once that's available.
          initialDelaySeconds: 3
          timeoutSeconds: 5
        args:
        - --domain=cluster.local.
        - --dns-port=10053
        - --config-map=kube-dns
        - --kube-master-url=http://10.36.10.18:8080
        # This should be set to v=2 only after the new image (cut from 1.5) has
        # been released, otherwise we will flood the logs.
        - --v=0
        # __PILLAR__FEDERATIONS__DOMAIN__MAP__
        env:
        - name: PROMETHEUS_PORT
          value: &quot;10055&quot;
        ports:
        - containerPort: 10053
          name: dns-local
          protocol: UDP
        - containerPort: 10053
          name: dns-tcp-local
          protocol: TCP
        - containerPort: 10055
          name: metrics
          protocol: TCP
      - name: dnsmasq
        image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /healthz-dnsmasq
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        args:
        - --cache-size=1000
        - --no-resolv
        - --server=127.0.0.1#10053
        - --log-facility=-
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
        resources:
          requests:
            cpu: 150m
            memory: 10Mi
      - name: dnsmasq-metrics
        image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /metrics
            port: 10054
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        args:
        - --v=2
        - --logtostderr
        ports:
        - containerPort: 10054
          name: metrics
          protocol: TCP
        resources:
          requests:
            memory: 10Mi
      - name: healthz
        image: gcr.io/google_containers/exechealthz-amd64:1.2
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 50Mi
          requests:
            cpu: 10m
            # Note that this container shouldn't really need 50Mi of memory. The
            # limits are set higher than expected pending investigation on #29688.
            # The extra memory was stolen from the kubedns container to keep the
            # net memory requested by the pod constant.
            memory: 50Mi
        args:
        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 &gt;/dev/null
        - --url=/healthz-dnsmasq
        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 &gt;/dev/null
        - --url=/healthz-kubedns
        - --port=8080
        - --quiet
        ports:
        - containerPort: 8080
          protocol: TCP
      dnsPolicy: Default  # Don't use cluster DNS.

</code></pre>
<p>skydns-svc.yaml文件如下：</p>
<pre><code># Copyright 2016 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*

# __MACHINE_GENERATED_WARNING__

apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: &quot;true&quot;
    kubernetes.io/name: &quot;KubeDNS&quot;
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.36.10.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP

</code></pre>
<p>修改ClusterIP参数为dns指定一个集群ip</p>
<h2 id="创建dns">创建dns</h2>
<pre><code>kubectl create -f skydns-rc.yaml
kubectl create -f skydns-svc.yaml
</code></pre>
<h2 id="重启k8s-nodes">重启k8s Nodes</h2>
<ol>
<li>修改各node节点上的/etc/kubernetes/kubelet配置文件</li>
</ol>
<pre><code>KUBELET_ARGS=&quot;--cluster_dns=10.36.10.2 --cluster_domain=cluster.local&quot;
</code></pre>
<p>--cluster_dns为指定的ip,</p>
<ol start="2">
<li>
<p>重启node</p>
<pre><code>for SERVICES in kube-proxy kubelet flanneld docker; do
    systemctl stop $SERVICES
done

iptables --flush
iptables -tnat --flush

for SERVICES in kube-proxy kubelet flanneld docker; do
    systemctl restart $SERVICES
    systemctl enable $SERVICES
    systemctl status $SERVICES
done
</code></pre>
</li>
</ol>
<h2 id="测试">测试</h2>
<p>添加一个busybox的pod用于测试，busybox.yaml如下</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: googlecontainer/busybox
    imagePullPolicy: IfNotPresent
    command:
      - sleep
      - &quot;3600&quot;
    name: busybox
  restartPolicy: Always
</code></pre>
<pre><code>kubectl exec -ti busybox -- nslookup kubernetes
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://caijh.github.io//post-images/1565334765466.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maven - 怎么样skip test]]></title>
        <id>https://caijh.github.io/post/maven-zen-me-yang-skip-test</id>
        <link href="https://caijh.github.io/post/maven-zen-me-yang-skip-test">
        </link>
        <updated>2019-08-06T03:20:36.000Z</updated>
        <summary type="html"><![CDATA[<h3 id="maven-参数设置">Maven 参数设置</h3>
<ul>
<li>使用参数-Dmaven.test.skip=true</li>
<li>在pom.xml中定义参数<br>
<code>&lt;properties&gt; &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt; &lt;/properties&gt;</code></li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<h3 id="maven-参数设置">Maven 参数设置</h3>
<ul>
<li>使用参数-Dmaven.test.skip=true</li>
<li>在pom.xml中定义参数<br>
<code>&lt;properties&gt; &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt; &lt;/properties&gt;</code></li>
</ul>
<!-- more -->
<h3 id="maven-surefire-plugin">Maven Surefire Plugin</h3>
<pre><code>mvn package -DskipTests
</code></pre>
<p>or</p>
<pre><code>&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
    &lt;version&gt;3.0.0-M1&lt;/version&gt;
    &lt;configuration&gt;
        &lt;skipTests&gt;true&lt;/skipTests&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s - 基本概念]]></title>
        <id>https://caijh.github.io/post/k8s-ji-ben-gai-nian</id>
        <link href="https://caijh.github.io/post/k8s-ji-ben-gai-nian">
        </link>
        <updated>2019-08-06T02:57:14.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="pod">Pod</h2>
<p>Pod是一个或多个若干相关容器的组合，Pod包含的容器运行在同一台宿主机上，这些容器使用相同的网络命名空间、IP地址和端口，相互之间能通过localhost来发现和通信。另外，这些容器还可共享一块存储卷空间。在Kubernetes中创建、调度和管理的最小单位是Pod，而不是容器，Pod通过提供更高层次的抽象，提供了更加灵活的部署和管理模式。</p>
<p>Pod是Kubernetes的基本操作单元，也是应用运行的载体。</p>
<p>Pod是容器的集合，容器是真正的执行体。相比原生的容器接口，Pod提供了更高层次的抽象，但是Pod的设计并不是为了运行同一个应用的多个实例，而是运行一个应用多个紧密联系的程序。而每个程序运行在单独的容器中，以Pod的形式组合成一个应用。</p>
<h2 id="replication-controller">Replication Controller</h2>
<p>Replication Controller用来控制管理Pod副本（Replica，或者称为实例），Replication Controller确保任何时候Kubernetes集群中有指定数量的Pod副本在运行。如果少于指定数量的Pod副本，Replication Controller会启动新的Pod副本，反之会杀死多余的副本以保证数量不变。另外，Replication Controller是弹性伸缩、滚动升级的实现核心。</p>
<p>注：官方建议使用Deployment代替</p>
<h2 id="service">Service</h2>
<p>Service是真实应用服务的抽象，定义了Pod的逻辑集合和访问这个Pod集合的策略。Service将代理Pod对外表现为一个单一访问接口，外部不需要了解后端Pod如何运行，这给扩展和维护带来很多好处，提供了一套简化的服务代理和发现机制。</p>
<h2 id="label">Label</h2>
<p>Label是用于区分Pod、Service、Replication Controller的Key/Value对，实际上，Kubernetes中的任意API对象都可以通过Label进行标识。每个API对象可以有多个Label，但是每个Label的Key只能对应一个Value。Label是Service和Replication Controller运行的基础，它们都通过Label来关联Pod.</p>
<h2 id="master">Master</h2>
<p>Kubernetes里的Master指的是集群控制节点，每个Kubernetes集群里需要有一个Master节点来负责整个集群的管理和控制，基本上Kubernetes所有的控制命令都是发给它，它来负责具体的执行过程.</p>
<p>在Master运行的组件有：</p>
<ul>
<li>kube-apiserver, 提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程.</li>
<li>etcd, 强一致性高可用的key-value存储系统，kubernetes用来存储集群的数据。</li>
<li>Kube-scheduler，负责资源调度（Pod调度）的进程，为新建的Pod分配Node。</li>
<li>Kube-controller-manager，Kubernetes里所有资源对象的自动化控制中心，负责执行各种控制器。</li>
</ul>
<h2 id="node">Node</h2>
<p>Kubernetes属于主从分布式集群架构，Kubernetes Node（简称为Node，早期版本叫作Minion）运行并管理容器。Node作为Kubernetes的操作单元，用来分配给Pod（或者说容器）进行绑定，Pod最终运行在Node上，Node可以认为是Pod的宿主机。</p>
<p>Node节点上运行的组件有：</p>
<ul>
<li>kubelet, 负责Pod对应的容器的创建、启停等任务，同时与Master节点密切协作，实现集群管理的基本功能。</li>
<li>Kube-proxy, 实现Kubernetes Service的通信与负载均衡机制的重要组件。</li>
<li>container runtime, 负责运行容器。</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<h2 id="pod">Pod</h2>
<p>Pod是一个或多个若干相关容器的组合，Pod包含的容器运行在同一台宿主机上，这些容器使用相同的网络命名空间、IP地址和端口，相互之间能通过localhost来发现和通信。另外，这些容器还可共享一块存储卷空间。在Kubernetes中创建、调度和管理的最小单位是Pod，而不是容器，Pod通过提供更高层次的抽象，提供了更加灵活的部署和管理模式。</p>
<p>Pod是Kubernetes的基本操作单元，也是应用运行的载体。</p>
<p>Pod是容器的集合，容器是真正的执行体。相比原生的容器接口，Pod提供了更高层次的抽象，但是Pod的设计并不是为了运行同一个应用的多个实例，而是运行一个应用多个紧密联系的程序。而每个程序运行在单独的容器中，以Pod的形式组合成一个应用。</p>
<h2 id="replication-controller">Replication Controller</h2>
<p>Replication Controller用来控制管理Pod副本（Replica，或者称为实例），Replication Controller确保任何时候Kubernetes集群中有指定数量的Pod副本在运行。如果少于指定数量的Pod副本，Replication Controller会启动新的Pod副本，反之会杀死多余的副本以保证数量不变。另外，Replication Controller是弹性伸缩、滚动升级的实现核心。</p>
<p>注：官方建议使用Deployment代替</p>
<h2 id="service">Service</h2>
<p>Service是真实应用服务的抽象，定义了Pod的逻辑集合和访问这个Pod集合的策略。Service将代理Pod对外表现为一个单一访问接口，外部不需要了解后端Pod如何运行，这给扩展和维护带来很多好处，提供了一套简化的服务代理和发现机制。</p>
<h2 id="label">Label</h2>
<p>Label是用于区分Pod、Service、Replication Controller的Key/Value对，实际上，Kubernetes中的任意API对象都可以通过Label进行标识。每个API对象可以有多个Label，但是每个Label的Key只能对应一个Value。Label是Service和Replication Controller运行的基础，它们都通过Label来关联Pod.</p>
<h2 id="master">Master</h2>
<p>Kubernetes里的Master指的是集群控制节点，每个Kubernetes集群里需要有一个Master节点来负责整个集群的管理和控制，基本上Kubernetes所有的控制命令都是发给它，它来负责具体的执行过程.</p>
<p>在Master运行的组件有：</p>
<ul>
<li>kube-apiserver, 提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程.</li>
<li>etcd, 强一致性高可用的key-value存储系统，kubernetes用来存储集群的数据。</li>
<li>Kube-scheduler，负责资源调度（Pod调度）的进程，为新建的Pod分配Node。</li>
<li>Kube-controller-manager，Kubernetes里所有资源对象的自动化控制中心，负责执行各种控制器。</li>
</ul>
<h2 id="node">Node</h2>
<p>Kubernetes属于主从分布式集群架构，Kubernetes Node（简称为Node，早期版本叫作Minion）运行并管理容器。Node作为Kubernetes的操作单元，用来分配给Pod（或者说容器）进行绑定，Pod最终运行在Node上，Node可以认为是Pod的宿主机。</p>
<p>Node节点上运行的组件有：</p>
<ul>
<li>kubelet, 负责Pod对应的容器的创建、启停等任务，同时与Master节点密切协作，实现集群管理的基本功能。</li>
<li>Kube-proxy, 实现Kubernetes Service的通信与负载均衡机制的重要组件。</li>
<li>container runtime, 负责运行容器。</li>
</ul>
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringBoot - 启动原理]]></title>
        <id>https://caijh.github.io/post/springboot-qi-dong-yuan-li</id>
        <link href="https://caijh.github.io/post/springboot-qi-dong-yuan-li">
        </link>
        <updated>2019-07-20T03:41:54.000Z</updated>
        <summary type="html"><![CDATA[<p>SpringBoot应用启动类一般如下：</p>
<pre><code>@SpringBootApplication
public class SampleRabbitmqApplication {

    public static void main(String[] args) {
        SpringApplication.run(SampleRabbitmqApplication.class, args);
    }

}
</code></pre>
<h1 id="springbootapplication">@SpringBootApplication</h1>
<p>@SpringBootApplication这个组合注解包含了三个注解 ：</p>
<ul>
<li>@SpringBootConfiguration</li>
<li>@EnableAutoConfiguration</li>
<li>@ComponentScan</li>
</ul>
<p>@SpringBootConfiguration实际上应用@Configuration， 任何注解了Configuration了java类都是一个JavaConfig配置类。<br>
@ComponentScan的功能是自动扫描并加载符合条件的组件（如@Component和@Repository）,最终将这些bean加载IoC容器中。<br>
@EnableAutoConfiguration借助@Import收集和注册特定场景相关的bean定义。</p>
]]></summary>
        <content type="html"><![CDATA[<p>SpringBoot应用启动类一般如下：</p>
<pre><code>@SpringBootApplication
public class SampleRabbitmqApplication {

    public static void main(String[] args) {
        SpringApplication.run(SampleRabbitmqApplication.class, args);
    }

}
</code></pre>
<h1 id="springbootapplication">@SpringBootApplication</h1>
<p>@SpringBootApplication这个组合注解包含了三个注解 ：</p>
<ul>
<li>@SpringBootConfiguration</li>
<li>@EnableAutoConfiguration</li>
<li>@ComponentScan</li>
</ul>
<p>@SpringBootConfiguration实际上应用@Configuration， 任何注解了Configuration了java类都是一个JavaConfig配置类。<br>
@ComponentScan的功能是自动扫描并加载符合条件的组件（如@Component和@Repository）,最终将这些bean加载IoC容器中。<br>
@EnableAutoConfiguration借助@Import收集和注册特定场景相关的bean定义。</p>
<!-- more -->
<pre><code>@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@AutoConfigurationPackage
@Import(AutoConfigurationImportSelector.class)
public @interface EnableAutoConfiguration {

	String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;;

	/**
	 * Exclude specific auto-configuration classes such that they will never be applied.
	 * @return the classes to exclude
	 */
	Class&lt;?&gt;[] exclude() default {};

	/**
	 * Exclude specific auto-configuration class names such that they will never be
	 * applied.
	 * @return the class names to exclude
	 * @since 1.3.0
	 */
	String[] excludeName() default {};

}
</code></pre>
<p>@Import(AutoConfigurationImportSelector.class)中借助AutoConfigurationImportSelector可以帮助SpringBoot应用将所有符合@Configuration配置都加载到当SpringBoot创建IoC容器中。AutoConfigurationImportSelector使用SpringFactoriesLoader从META-INF/spring.factories加载配置。</p>
<h4 id="springapplicationrun">SpringApplication.run</h4>
<pre><code>public ConfigurableApplicationContext run(String... args) {
		StopWatch stopWatch = new StopWatch();
		stopWatch.start();
		ConfigurableApplicationContext context = null;
		Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;();
		configureHeadlessProperty();
		SpringApplicationRunListeners listeners = getRunListeners(args);
		listeners.starting();
		try {
			ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);
			ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments);
			configureIgnoreBeanInfo(environment);
			Banner printedBanner = printBanner(environment);
			context = createApplicationContext();
			exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class,
					new Class[] { ConfigurableApplicationContext.class }, context);
			prepareContext(context, environment, listeners, applicationArguments, printedBanner);
			refreshContext(context);
			afterRefresh(context, applicationArguments);
			stopWatch.stop();
			if (this.logStartupInfo) {
				new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch);
			}
			listeners.started(context);
			callRunners(context, applicationArguments);
		}
		catch (Throwable ex) {
			handleRunFailure(context, ex, exceptionReporters, listeners);
			throw new IllegalStateException(ex);
		}

		try {
			listeners.running(context);
		}
		catch (Throwable ex) {
			handleRunFailure(context, ex, exceptionReporters, null);
			throw new IllegalStateException(ex);
		}
		return context;
	}
</code></pre>
<ol>
<li>创建SpringApplicationRunListeners, 获取SpringFactoriesInstances实例</li>
<li>准备环境变量</li>
<li>是否打印banner</li>
<li>创建ApplicationContext,servelt环境创建AnnotationConfigServletWebServerApplicationContext， reactive环境创建AnnotationConfigReactiveWebServerApplicationContext， 否则创建AnnotationConfigApplicationContext</li>
<li>prepareContext, 主要是ApplicationContextInitializer的初始化</li>
<li>refreshContext 调用ApplicationContext的refresh()方法</li>
<li>afterRefresh, 空方法</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redis - 编译纯净版]]></title>
        <id>https://caijh.github.io/post/redis-bian-yi-chun-jing-ban</id>
        <link href="https://caijh.github.io/post/redis-bian-yi-chun-jing-ban">
        </link>
        <updated>2019-07-18T07:38:08.000Z</updated>
        <summary type="html"><![CDATA[<p>按redis官网编译的redis, 可执行文件会与源码在一起, 如果想编译纯净版可按下边的步骤。</p>
]]></summary>
        <content type="html"><![CDATA[<p>按redis官网编译的redis, 可执行文件会与源码在一起, 如果想编译纯净版可按下边的步骤。</p>
<!-- more -->
<h3 id="下载redis">下载redis</h3>
<pre><code>$ wget http://download.redis.io/releases/redis-5.0.5.tar.gz
$ tar xzf redis-5.0.5.tar.gz
$ cd redis-5.0.5
</code></pre>
<h3 id="make-指定rpefix值">make 指定RPEFIX值</h3>
<pre><code>make PREFIX=~/dev/redis-5.0.5 install
</code></pre>
<h3 id="复制配置文件到安装目录">复制配置文件到安装目录</h3>
<pre><code>➜  redis-5.0.5 cp redis.conf ~/dev/redis-5.0.5 
➜  redis-5.0.5 cp sentinel.conf ~/dev/redis-5.0.5
</code></pre>
]]></content>
    </entry>
</feed>